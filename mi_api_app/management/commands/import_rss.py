from django.core.management.base import BaseCommand
from mi_api_app.models import noticias
from mi_api_app.feedrss import get_news_feed
import json
from datetime import datetime
import html
import re

class Command(BaseCommand):
    help = 'Importa noticias desde m√∫ltiples feeds RSS a la base de datos'

    def add_arguments(self, parser):
        parser.add_argument('--limit', type=int, default=5, help='L√≠mite de noticias por feed')
        parser.add_argument('--url', type=str, help='URL espec√≠fica de un feed (opcional)')
        parser.add_argument('--skip-errors', action='store_true', help='Omitir feeds con errores')

    def truncate_string(self, text, max_length=200):
        """Trunca una cadena a la longitud m√°xima especificada"""
        if text and len(text) > max_length:
            return text[:max_length-3] + '...'
        return text

    def clean_html(self, raw_html):
        """Limpia HTML y elimina etiquetas"""
        if not raw_html:
            return ""
        # Primero decodificar entidades HTML
        clean_text = html.unescape(raw_html)
        # Eliminar etiquetas HTML
        clean_text = re.sub('<[^<]+?>', '', clean_text)
        # Eliminar espacios m√∫ltiples y saltos de l√≠nea
        clean_text = re.sub('\s+', ' ', clean_text).strip()
        return clean_text

    def handle(self, *args, **options):
        urls = [
            "http://feeds.bbci.co.uk/news/world/rss.xml",                 # BBC World
            "https://rss.cnn.com/rss/edition_world.rss",                  # CNN World
            "https://www.aljazeera.com/xml/rss/all.xml",                  # Al Jazeera
            "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",     # The New York Times - World
            "https://www.theguardian.com/world/rss",                      # The Guardian - World
            "https://www.reutersagency.com/feed/?best-topics=world",      # Reuters World
            "https://feeds.nbcnews.com/nbcnews/public/world",             # NBC News
            "https://feeds.a.dj.com/rss/RSSWorldNews.xml",                # Wall Street Journal - World
            "https://www.ft.com/rss/world",                               # Financial Times
            "https://rss.politico.com/playbook.xml",                # Politico World
            "https://www.france24.com/en/rss",                            # France24 English
            "https://english.kyodonews.net/rss/news",                     # Kyodo News (Jap√≥n)
            "https://www.dw.com/atom/rss-en-all",                         # Deutsche Welle (Alemania)
            "https://www.npr.org/rss/rss.php?id=1004",                    # NPR - World

            # üá¶üá∑ ARGENTINA
            "https://www.clarin.com/rss/lo-ultimo/",                      # Clar√≠n - Lo √öltimo
            "https://www.clarin.com/rss/mundo/",                          # Clar√≠n - Mundo
            "https://www.lanacion.com.ar/rssfeed/",                       # La Naci√≥n - General
            "https://www.pagina12.com.ar/rss/portada",                    # P√°gina/12 - Portada
            "https://www.perfil.com/rss/feed.xml",                        # Perfil - General
            "https://www.lavoz.com.ar/rss/ultimas-noticias.xml",          # La Voz del Interior
            "https://www.ambito.com/rss/ultimas-noticias.xml",            # √Åmbito Financiero
            "https://www.cronista.com/files/rss/ultimas-noticias.xml",    # El Cronista
            "https://www.telam.com.ar/rss2/ultimasnoticias.xml",          # T√©lam
            "https://www.rionegro.com.ar/feed/",                          # Diario R√≠o Negro
            "https://www.lagaceta.com.ar/rss/rss.xml",                    # La Gaceta (Tucum√°n)
            "https://www.losandes.com.ar/feed/",                          # Los Andes (Mendoza)
            "https://www.diariouno.com.ar/feed",                          # Diario Uno (Mendoza)
            "https://www.eldia.com/rss/ultimas-noticias.xml",             # Diario El D√≠a (La Plata)
            "https://www.baenegocios.com/rss/feed.xml",                   # BAE Negocios
            "https://www.minutouno.com/rss",                              # Minuto Uno

            # üáßüá∑ BRASIL
            "https://g1.globo.com/rss/g1/",                               # G1 (Globo)
            "https://feeds.folha.uol.com.br/emcimadahora/rss091.xml",     # Folha de S√£o Paulo
            "https://rss.uol.com.br/feed/noticias.xml",                   # UOL Not√≠cias

            # üá≤üáΩ M√âXICO              
            "https://www.jornada.com.mx/rss/edicion.xml",                 # La Jornada
            "https://www.milenio.com/rss",               # Milenio

            # üá™üá∏ ESPA√ëA
            "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/portada",  # El Pa√≠s - Portada
            "https://ep00.epimg.net/rss/elpais/internacional.xml",        # El Pa√≠s - Internacional
            "https://www.abc.es/rss/feeds/abc_ultima.xml",                # ABC Espa√±a
            "https://e00-elmundo.uecdn.es/elmundo/rss/portada.xml",       # El Mundo

            # üì∞ GOOGLE NEWS (cambia "hl", "gl", "ceid" seg√∫n idioma/pa√≠s)
            "https://news.google.com/rss?hl=en-US&gl=US&ceid=US:en",      # USA (Ingl√©s)
            "https://news.google.com/rss?hl=en-GB&gl=GB&ceid=GB:en",      # Reino Unido
            "https://news.google.com/rss?hl=en-IN&gl=IN&ceid=IN:en",      # India (Ingl√©s)
            "https://news.google.com/rss?hl=es-419&gl=AR&ceid=AR:es-419", # Argentina
            "https://news.google.com/rss?hl=es-419&gl=MX&ceid=MX:es-419", # M√©xico
            "https://news.google.com/rss?hl=es&gl=ES&ceid=ES:es",         # Espa√±a
            "https://news.google.com/rss?hl=pt-BR&gl=BR&ceid=BR:pt-419",  # Brasil
            "https://news.google.com/rss?hl=fr&gl=FR&ceid=FR:fr",         # Francia
            "https://news.google.com/rss?hl=de&gl=DE&ceid=DE:de",         # Alemania
            "https://news.google.com/rss?hl=it&gl=IT&ceid=IT:it",         # Italia
            "https://news.google.com/rss?hl=ru&gl=RU&ceid=RU:ru",         # Rusia
            "https://news.google.com/rss?hl=ja&gl=JP&ceid=JP:ja",         # Jap√≥n
            "https://news.google.com/rss?hl=zh-CN&gl=CN&ceid=CN:zh-Hans", # China (Simplificado)
            "https://news.google.com/rss?hl=zh-TW&gl=TW&ceid=TW:zh-Hant", # Taiw√°n (Tradicional)
            "https://news.google.com/rss?hl=ar&gl=EG&ceid=EG:ar",         # Medio Oriente (Egipto - √Årabe)
            "https://news.google.com/rss?hl=en-AU&gl=AU&ceid=AU:en",      # Australia
            "https://news.google.com/rss?hl=en-CA&gl=CA&ceid=CA:en",      # Canad√° (Ingl√©s)
            "https://news.google.com/rss?hl=fr-CA&gl=CA&ceid=CA:fr",      # Canad√° (Franc√©s)
            "https://news.google.com/rss?hl=ko&gl=KR&ceid=KR:ko",         # Corea del Sur    
        ]
        
        limit = options['limit']
        specific_url = options['url']
        skip_errors = options['skip_errors']
        
        # Si se especifica una URL, usar solo esa
        if specific_url:
            urls = [specific_url]
        else:
            # Filtrar URLs problem√°ticas si se solicita omitir errores
            if skip_errors:
                urls = [url for url in urls if url not in problematic_urls]
        
        total_imported = 0
        total_processed = 0
        failed_feeds = 0
        
        for i, url in enumerate(urls, 1):
            self.stdout.write(f"\nüì° Procesando feed {i}/{len(urls)}: {url}")
            
            try:
                # Obtener noticias del RSS
                news_json = get_news_feed(url, limit)
                news_list = json.loads(news_json)
                
                count = 0
                for news_item in news_list:
                    # Limpiar y truncar campos si es necesario
                    titulo = self.truncate_string(self.clean_html(news_item.get('title', '')))
                    descripcion = self.truncate_string(self.clean_html(news_item.get('description', '')))
                    link = self.truncate_string(news_item.get('link', ''), 500)  # Links pueden ser largos
                    
                    # Verificar si la noticia ya existe para evitar duplicados
                    if not noticias.objects.filter(link=link).exists():
                        noticias.objects.create(
                            titulo=titulo,
                            descripcion=descripcion,
                            site_icon=news_item.get('site_icon', ''),
                            link=link,
                            image=news_item.get('image', ''),
                        )
                        count += 1
                
                total_imported += count
                total_processed += len(news_list)
                
                self.stdout.write(
                    self.style.SUCCESS(
                        f'‚úÖ Importadas {count} nuevas noticias de {len(news_list)} encontradas'
                    )
                )
                
            except Exception as e:
                failed_feeds += 1
                self.stdout.write(
                    self.style.ERROR(
                        f'‚ùå Error procesando {url}: {str(e)}'
                    )
                )
                # Si es una URL espec√≠fica, probablemente queremos ver el error
                if specific_url:
                    raise e
                continue
        
        # Resumen final
        self.stdout.write("\n" + "="*50)
        self.stdout.write(
            self.style.SUCCESS(
                f'üéØ RESUMEN FINAL: Importadas {total_imported} nuevas noticias '
                f'de {total_processed} procesadas desde {len(urls)} feeds'
            )
        )
        if failed_feeds > 0:
            self.stdout.write(
                self.style.WARNING(
                    f'‚ö†Ô∏è  {failed_feeds} feeds tuvieron errores durante el procesamiento'
                )
            )